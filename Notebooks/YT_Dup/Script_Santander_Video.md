Competition Objective:

Santander Group is asking Data Scientist to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.

Lets the get the Business Context a little bit here.
One of the paramount problems in Banking is predicting the value of transactions for each potential customer.

Becuase if as a Banker I can predict the number or value tha amount of Transactions that a Customer will bring to the Bank, then this value can become the basis on which other personalized services can be provided.

-----------------


Now coming back to the specific challenge here, This Value of transactions is a continuous variable and its prediction is a regression problem.

The Training dataset contains a target variable (which is the value of transaction per customer).

And speciality of this problem here is that we are dealing with a “sparse tabular dataset” and which is completely anonymised. This makes it very difficult to exploit domain knowledge.

Means, here, the Feature variables come with no description whatsoever and with anonymized names like “48df886f9” that provide little meaning as to what they represent.

--

One can guess that they are possibly aggregated transactions (by day, by week, by month…) or an individual transaction which could be further grouped by some financial instrument (debit, credit, loan, …).

So as a Data Scientist no one would like this kind of anonymized data. We would like to know the feature name and their clear definitions etc.

But even the Competition host the Santander Group cant help much here. As for most companies, the data they possess is a valuable asset that cannot be compromised.

This is probably the best that Santander can do, or the best that their legal/compliance team would approve of. It would either be obfuscated data or no data at all that can be made publicly available. So we have to work with this constraint here.

Hence the point is, this competition doesn't seem to offer a lot of room for either sophisticated models or smart feature engineering employing domian-knowledge becaue all the features are anonymized. If no data-leak or other similar kind of "property" is found in the dataset, the winning solution has all chances to be just a very deep stack of lgbm/xgboost/nn models.


## Problem Statement

This project aims to solve a high-dimensional sparse regression problem. The variable that we attempt to predict is value of transactions for each potential customer from 4991 feature variables.

# RMSE vs RMSLE

1. In the case of RMSE, the presence of outliers can explode the error term to a very high value. But, in the case of RMLSE the outliers are drastically scaled down therefore nullifying their effect.

Evaluation metric RMSLE is usually used when you don’t want to penalize huge differences in the predicted and the actual values and when both predicted and the actual values are large numbers.

RMSE explodes in magnitude as soon as it encounters an outlier. In contrast, even on the introduction of the outlier, the RMSLE error is not affected much

2. RMSLE metric only considers the relative error between the Predicted and the actual value and the scale of the error is not significant. On the other hand, RMSE value Increases in magnitude if the scale of error increases.


So RMSLE especially useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated.


===========


 the data seems very "massaged" prior to us getting our hands on it, and by this I mean, PCA and autoencoders may not add much value since features don't show obvious correlation. It's as if they are already "fully compressed."  In other words, they are already statistically independent and seem to contain little, if any, redundant information (as if principal component analysis has already been performed).

 But then, I also don't think the data is a result of of PCA because of how diverse the features distributions are. Some are negative, some have spikes some don't.

 ====


 **What is  Boosting**

To understand the absolute basics of the need for Boosting algorithm, lets ask a basic question - If a data point is incorrectly predicted by our first model, and then the next (probably all models), will combining the predictions provide better results? Such questions are handled by boosting algorithm.

So, Boosting is a sequential technique which works on the principle of an ensemble, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.

The basic principle behind the working of the boosting algorithm is to generate multiple weak learners and combine their predictions to form one strong rule. These weak rules are generated by applying base Machine Learning algorithms on different distributions of the data set. These algorithms generate weak rules for each iteration. After multiple iterations, the weak learners are combined to form a strong learner that will predict a more accurate outcome.
 Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%.
